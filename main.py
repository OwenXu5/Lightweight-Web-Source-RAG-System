import faiss
import numpy as np
from dotenv import load_dotenv
import os
import zipfile
import pickle
import httpx
import json
from typing import List, Optional, Tuple
from tqdm import tqdm

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from openai import OpenAI

load_dotenv()

REBUILD_FLAG = os.getenv("REBUILD_FLAG", "False").lower() == "true"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")
VECTOR_DIR = os.getenv("VECTOR_DIR", "wiki_vector_store")

# Step 1: è§£å‹faisså‘é‡åº“ï¼ˆè‹¥å­˜åœ¨zipåˆ™è§£å‹ï¼‰
VECTOR_INDEX_FILE = os.path.join(VECTOR_DIR, "faiss.index")
ID2CONTENT_FILE = os.path.join(VECTOR_DIR, "id2content.pkl")
ID2META_FILE = os.path.join(VECTOR_DIR, "id2meta.pkl")
ID2RAW_FILE = os.path.join(VECTOR_DIR, "id2raw.pkl")
ID2TITLE_FILE = os.path.join(VECTOR_DIR, "id2title.pkl")

if not os.path.exists(VECTOR_DIR) and os.path.exists("wiki_allinone.zip"):
    with zipfile.ZipFile("wiki_allinone.zip", "r") as zip_ref:
        zip_ref.extractall(VECTOR_DIR)


def load_urls_from_file(path: str) -> List[str]:
    urls = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            urls.append(line)
    if not urls:
        raise ValueError(f"æ–‡ä»¶ {path} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URL")
    print(f"åŠ è½½äº† {len(urls)} ä¸ªURL")
    return urls


def load_web_documents(urls: List[str]):
    """ä½¿ç”¨ WebBaseLoader æŠ“å–ç½‘é¡µä¸º Document åˆ—è¡¨"""
    docs = []
    for url in tqdm(urls, desc="åŠ è½½ç½‘é¡µæ–‡æ¡£"):
        loader = WebBaseLoader(url)
        docs.extend(loader.load())
    return docs


def split_documents(documents, chunk_size: int = 800, chunk_overlap: int = 80):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(documents)


def embed_texts(texts: List[str], model: str = "ecnu-embedding-small") -> np.ndarray:
    """æ‰¹é‡è·å–æ–‡æœ¬å‘é‡ï¼Œä½¿ç”¨å­¦æ ¡å¹³å°çš„embeddingæ¥å£"""
    emb_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
    response = emb_client.embeddings.create(input=texts, model=model)
    vectors = [item.embedding for item in response.data]
    return np.array(vectors, dtype=np.float32)


def build_faiss_from_documents(
    documents, chunk_size: int = 800, chunk_overlap: int = 80
) -> Tuple[faiss.IndexFlatL2, dict, dict, dict, dict]:
    """ä» Document åˆ—è¡¨æ„å»º FAISS ç´¢å¼•å¹¶è¿”å›æ˜ å°„"""
    splits = split_documents(
        documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    print(f"åˆ†å—åå…± {len(splits)} ä¸ªç‰‡æ®µ")
    texts = [d.page_content for d in tqdm(splits)]
    print(f"åµŒå…¥æ–‡æœ¬ä¸­...")
    vectors = embed_texts(texts)
    print(f"åµŒå…¥å®Œæˆï¼Œå‘é‡ç»´åº¦: {vectors.shape[1]}")

    # å»ºç«‹ FAISS ç´¢å¼•
    dim = vectors.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(vectors)

    # æ„å»ºæ˜ å°„
    id2content = {i: text for i, text in enumerate(texts)}
    id2raw = id2content.copy()
    id2title = {i: d.metadata.get("source", "") for i, d in enumerate(splits)}
    id2meta = {i: d.metadata for i, d in enumerate(splits)}

    return index, id2content, id2meta, id2raw, id2title


def save_vector_store(
    index: faiss.IndexFlatL2,
    id2content: dict,
    id2meta: dict,
    id2raw: dict,
    id2title: dict,
):
    os.makedirs(VECTOR_DIR, exist_ok=True)
    faiss.write_index(index, VECTOR_INDEX_FILE)
    with open(ID2CONTENT_FILE, "wb") as f:
        pickle.dump(id2content, f)
    with open(ID2META_FILE, "wb") as f:
        pickle.dump(id2meta, f)
    with open(ID2RAW_FILE, "wb") as f:
        pickle.dump(id2raw, f)
    with open(ID2TITLE_FILE, "wb") as f:
        pickle.dump(id2title, f)


def ensure_vector_store(urls: Optional[List[str]] = None, rebuild: bool = False):
    """ç¡®ä¿å‘é‡åº“å¯ç”¨ï¼›ç¼ºå¤±æˆ–æŒ‡å®šrebuildæ—¶ä»URLé‡å»º"""
    files_exist = all(
        os.path.exists(p)
        for p in [
            VECTOR_INDEX_FILE,
            ID2CONTENT_FILE,
            ID2META_FILE,
            ID2RAW_FILE,
            ID2TITLE_FILE,
        ]
    )
    if files_exist and not rebuild:
        return

    if not urls:
        raise ValueError("å‘é‡åº“ç¼ºå¤±ä¸”æœªæä¾› URL åˆ—è¡¨ï¼Œæ— æ³•æ„å»ºç´¢å¼•ã€‚")

    print("âš™ï¸ æ­£åœ¨ä»ç½‘é¡µé‡å»ºå‘é‡åº“...")
    documents = load_web_documents(urls)
    index, id2content, id2meta, id2raw, id2title = build_faiss_from_documents(documents)
    save_vector_store(index, id2content, id2meta, id2raw, id2title)
    print("âœ… å‘é‡åº“å·²æ„å»ºå¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚")


# Step 2: åŠ è½½Faissç´¢å¼•å’Œæ–‡æ¡£
def load_faiss_index():
    """åŠ è½½FAISSç´¢å¼•å’Œç›¸å…³çš„æ–‡æ¡£æ•°æ®"""
    index = faiss.read_index(VECTOR_INDEX_FILE)
    with open(ID2CONTENT_FILE, "rb") as f:
        id2content = pickle.load(f)
    with open(ID2META_FILE, "rb") as f:
        id2meta = pickle.load(f)
    with open(ID2RAW_FILE, "rb") as f:
        id2raw = pickle.load(f)
    with open(ID2TITLE_FILE, "rb") as f:
        id2title = pickle.load(f)

    return index, id2content, id2meta, id2raw, id2title


# é»˜è®¤ç¤ºä¾‹URL
DEFAULT_URLS = [
    "https://rag.deeptoai.com/docs/advanced-rag-intro/complete-rag-survey"
]


# Step 3: è·å¾—Embeddingæ–¹æ³•ï¼ˆç”¨å­¦æ ¡å¹³å° embedding æ¥å£ï¼‰
def get_embedding(text, model="ecnu-embedding-small"):
    emb_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)
    response = emb_client.embeddings.create(input=text, model=model)
    return np.array(response.data[0].embedding, dtype=np.float32)


# Step 4: æ£€ç´¢ç›¸å…³æ–‡æ¡£
def search(query, top_k=15):
    """æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œè¿”å›æ–‡æ¡£å†…å®¹åˆ—è¡¨å’Œç´¢å¼•ä¿¡æ¯"""
    query_emb = get_embedding(query)
    query_emb = np.expand_dims(query_emb, axis=0)
    D, I = index.search(query_emb, top_k)
    results = []
    indices = []
    for idx, distance in zip(I[0], D[0]):
        # å°è¯•ä¸åŒçš„ç´¢å¼•æ ¼å¼ï¼ˆæ•´æ•°ã€å­—ç¬¦ä¸²ï¼‰
        content = None
        if idx in id2content:
            content = id2content[idx]
        elif str(idx) in id2content:
            content = id2content[str(idx)]
        elif idx in id2raw:
            content = id2raw[idx]
        elif str(idx) in id2raw:
            content = id2raw[str(idx)]
        else:
            content = ""
        if content:  # åªæ·»åŠ éç©ºå†…å®¹
            results.append(content)
            indices.append((idx, float(distance)))
    return results, indices


# Step 4.5: Rerank é‡æ’åºæ¨¡å—
def rerank(query, documents, top_k=None, model="ecnu-rerank"):
    """
    ä½¿ç”¨ä¸“ç”¨çš„rerank APIå¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’åº

    Args:
        query: æŸ¥è¯¢é—®é¢˜
        documents: æ–‡æ¡£åˆ—è¡¨
        top_k: è¿”å›å‰kä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›å…¨éƒ¨
        model: rerankæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º"ecnu-rerank"

    Returns:
        é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
    """
    if not documents:
        return []

    if len(documents) == 1:
        return documents

    # å¦‚æœæ–‡æ¡£æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥è¿”å›
    if len(documents) <= 2:
        if top_k is not None:
            return documents[:top_k]
        return documents

    # ä½¿ç”¨ä¸“ç”¨çš„rerank API
    try:
        # æ„å»ºè¯·æ±‚æ•°æ®
        top_n = top_k if top_k is not None else len(documents)

        request_data = {
            "documents": documents,
            "model": model,
            "query": query,
            "return_documents": True,
            "top_n": top_n,
        }

        # ä½¿ç”¨httpxå‘é€POSTè¯·æ±‚
        with httpx.Client() as client:
            response = client.post(
                f"{OPENAI_API_BASE}/rerank",
                json=request_data,
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json",
                },
                timeout=30.0,
            )
            response.raise_for_status()
            result = response.json()

        # æ ¹æ®APIå“åº”æ ¼å¼æå–é‡æ’åºåçš„æ–‡æ¡£
        if "results" in result and isinstance(result["results"], list):
            # æŒ‰relevance_scoreæ’åºï¼ˆé€šå¸¸APIå·²ç»æ’åºï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§å†æ’åºä¸€æ¬¡ï¼‰
            sorted_results = sorted(
                result["results"],
                key=lambda x: x.get("relevance_score", 0),
                reverse=True,
            )

            # å¦‚æœreturn_documents=Trueï¼Œç»“æœä¸­å¯èƒ½åŒ…å«æ–‡æ¡£å†…å®¹
            reranked_docs = []
            for item in sorted_results:
                # ä¼˜å…ˆä½¿ç”¨è¿”å›çš„æ–‡æ¡£å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "document" in item:
                    reranked_docs.append(item["document"])
                elif "index" in item:
                    # å¦åˆ™ä½¿ç”¨ç´¢å¼•ä»åŸå§‹æ–‡æ¡£åˆ—è¡¨ä¸­è·å–
                    idx = item["index"]
                    if 0 <= idx < len(documents):
                        reranked_docs.append(documents[idx])
        elif "data" in result:
            # å¦‚æœè¿”å›äº†æ–‡æ¡£åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
            if isinstance(result["data"], list):
                reranked_docs = result["data"]
            else:
                reranked_docs = documents[:top_n] if top_k is not None else documents
        else:
            # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œä½¿ç”¨åŸå§‹é¡ºåº
            reranked_docs = documents[:top_n] if top_k is not None else documents

        return reranked_docs

    except Exception as e:
        print(f"Rerankè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}ï¼Œä½¿ç”¨åŸå§‹é¡ºåº")
        # å¦‚æœrerankå¤±è´¥ï¼Œè¿”å›åŸå§‹é¡ºåº
        if top_k is not None:
            return documents[:top_k]
        return documents


# Step 5: æ„é€ RAGè°ƒç”¨LLMè¿›è¡Œé—®ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰
def retrieve_augmented_generation(
    question, top_k=10, rerank_top_k=5, use_rerank=True, chat_model="ecnu-max"
):
    """
    æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆæµå¼è¾“å‡ºï¼‰

    Args:
        question: ç”¨æˆ·é—®é¢˜
        top_k: åˆå§‹æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
        rerank_top_k: rerankåä¿ç•™çš„æ–‡æ¡£æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨top_k
        use_rerank: æ˜¯å¦ä½¿ç”¨rerankæ¨¡å—
        chat_model: èŠå¤©æ¨¡å‹åç§°

    Yields:
        str: æµå¼è¾“å‡ºçš„æ–‡æœ¬å—
    """
    # åˆå§‹æ£€ç´¢ï¼Œè·å–æ›´å¤šå€™é€‰æ–‡æ¡£ç”¨äºrerank
    initial_k = top_k * 2 if use_rerank else top_k
    top_docs, indices = search(question, top_k=initial_k)

    # ä½¿ç”¨rerankè¿›è¡Œé‡æ’åº
    if use_rerank and len(top_docs) > 1:
        final_k = rerank_top_k if rerank_top_k is not None else top_k
        top_docs = rerank(question, top_docs, top_k=final_k)
    elif not use_rerank:
        top_docs = top_docs[:top_k]

    # è¿‡æ»¤ç©ºå†…å®¹
    top_docs = [doc for doc in top_docs if doc]
    context = "\n\n".join(top_docs)
    prompt = f"""
    åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š
    æ–‡æ¡£ï¼š{context}
    
    é—®é¢˜ï¼š{question}
    
    ç­”æ¡ˆï¼š
    """
    chat_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)

    # ä½¿ç”¨æµå¼è¾“å‡º
    stream = chat_client.chat.completions.create(
        model="ecnu-max",
        messages=[
            {
                "role": "system",
                "content": """
                ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·åŸºäºç»™å‡ºçš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚
                è¦æ±‚ï¼š
                1. åªä½¿ç”¨æä¾›çš„æ–‡æ¡£å†…å®¹
                2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
                3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µ
                4. ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€å›ç­”
                """,
            },
            {"role": "user", "content": prompt},
        ],
        stream=True,  # å¯ç”¨æµå¼è¾“å‡º
    )

    # æµå¼è¿”å›æ–‡æœ¬å—
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            yield chunk.choices[0].delta.content


# Step 6: ç®€å•çš„æ£€ç´¢è¯„æµ‹ï¼ˆHit@kï¼‰
def run_retrieval_evaluation(top_k: int = 5):
    """
    åœ¨å›ºå®šé—®é¢˜é›†ä¸Šï¼Œå¯¹æ£€ç´¢é˜¶æ®µè¿›è¡Œç®€å•è¯„æµ‹ï¼Œè®¡ç®— Hit@kã€‚
    """

    eval_file = "eval_set.json"
    if not os.path.exists(eval_file):
        raise FileNotFoundError(f"Evaluation file {eval_file} does not exist.")

    with open(eval_file, "r", encoding="utf-8") as f:
        eval_set = json.load(f)

    num_questions = len(eval_set)
    hit_count = 0

    print(f"\nğŸ” Evaluating(Hit@{top_k}), {num_questions} questions in total...\n")

    for idx_q, item in enumerate(eval_set, start=1):
        q = item["question"]
        keywords = [kw.lower() for kw in item["keywords"]]
        results, _ = search(q, top_k=top_k)

        is_hit = False
        for doc in results:
            text_lower = doc.lower()
            if any(kw in text_lower for kw in keywords):
                is_hit = True
                break

        if is_hit:
            hit_count += 1
            status = "Hit"
        else:
            status = "Not Hit"

        print(f"[{idx_q}/{num_questions}] Quesion: {q}")
        print(f"    Answer: {status}")

    hit_at_k = hit_count / num_questions if num_questions > 0 else 0.0
    print(f"\nâœ… Hit@{top_k}: {hit_at_k:.3f} ï¼ˆ{hit_count}/{num_questions}ï¼‰\n")


if __name__ == "__main__":
    url_file = "urls.txt"  # ä½ å­˜æ”¾é“¾æ¥çš„æ–‡ä»¶

    urls = load_urls_from_file(url_file) if os.path.isfile("urls.txt") else DEFAULT_URLS
    ensure_vector_store(urls=urls, rebuild=REBUILD_FLAG)  # é‡å»ºå‘é‡åº“
    index, id2content, id2meta, id2raw, id2title = load_faiss_index()
    print("Welcome to the FAISS-based RAG QA system.")
    print("Two modes are available:")
    print("  1) Interactive QA")
    print("  2) Retrieval evaluation (Hit@k)")
    mode = input("Enter Mode Code (default: 1):").strip()

    if mode == "2":
        try:
            k_input = input("è¯·è¾“å…¥è¯„æµ‹ä½¿ç”¨çš„ kï¼ˆé»˜è®¤ 5ï¼‰ï¼š").strip()
            top_k = int(k_input) if k_input else 5
        except ValueError:
            top_k = 5
        run_retrieval_evaluation(top_k=top_k)
    else:
        while True:
            question = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆqé€€å‡ºï¼‰ï¼š")
            if question.strip().lower() == "q":
                break
            print("AIç­”æ¡ˆï¼š", end="", flush=True)
            # æµå¼è¾“å‡ºç­”æ¡ˆ
            for chunk in retrieve_augmented_generation(question):
                print(chunk, end="", flush=True)
            print()  # æ¢è¡Œ
